{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs       LICENSE  pyproject.toml\t test_event_flow.Ipynb\ttrain_flow.py\n",
      "dataloader    loss     README.md\t test_just_event.Ipynb\tutils\n",
      "eval_flow.py  models   requirements.txt  tools\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/doha/Documents/Event-analysis/hagenaar2021\n"
     ]
    }
   ],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.7.0\n",
      "  Using cached torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
      "Collecting torchvision==0.8.0\n",
      "  Using cached torchvision-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting PyYAML==5.3.1\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting numpy==1.19.2\n",
      "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting hdf5plugin==3.3.1\n",
      "  Using cached hdf5plugin-3.3.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.4.0.44 at https://files.pythonhosted.org/packages/01/d0/23b56d1c4a301d57ffa5d9aa0df9894053e43e439c9a710c8986ad7b3999/opencv_python-4.4.0.44-cp37-cp37m-manylinux2014_x86_64.whl#sha256=23dade76fe0194139112eea7ecdfa02ae09924b1d8d853f17f387a356519e484 (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
      "Reason for being yanked: deprecated, use 4.4.0.46\u001b[0m\n",
      "Collecting opencv-python==4.4.0.44\n",
      "  Using cached opencv_python-4.4.0.44-cp37-cp37m-manylinux2014_x86_64.whl (49.5 MB)\n",
      "Collecting pre-commit\n",
      "  Using cached pre_commit-2.21.0-py2.py3-none-any.whl (201 kB)\n",
      "Collecting matplotlib==3.3.2\n",
      "  Using cached matplotlib-3.3.2-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "Collecting progress\n",
      "  Using cached progress-1.6.tar.gz (7.8 kB)\n",
      "Collecting mlflow\n",
      "  Using cached mlflow-1.30.1-py3-none-any.whl (17.0 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting future\n",
      "  Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting pillow>=4.1.1\n",
      "  Using cached Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/doha/.pyenv/versions/3.7.3/envs/event_flow/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: six in /home/doha/.pyenv/versions/3.7.3/envs/event_flow/lib/python3.7/site-packages (from h5py==2.10.0->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting cfgv>=2.0.0\n",
      "  Using cached cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting virtualenv>=20.10.0\n",
      "  Using cached virtualenv-20.26.6-py3-none-any.whl (6.0 MB)\n",
      "Collecting identify>=1.0.0\n",
      "  Using cached identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting nodeenv>=0.11.1\n",
      "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Collecting certifi>=2020.06.20\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /home/doha/.pyenv/versions/3.7.3/envs/event_flow/lib/python3.7/site-packages (from mlflow->-r requirements.txt (line 12)) (0.4)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "Collecting gunicorn<21; platform_system != \"Windows\"\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Collecting Flask<3\n",
      "  Using cached Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "Collecting scipy<2\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting sqlalchemy<2,>=1.4.0\n",
      "  Using cached SQLAlchemy-1.4.54-cp37-cp37m-manylinux1_x86_64.manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_5_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting packaging<22\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Collecting requests<3,>=2.17.3\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Collecting alembic<2\n",
      "  Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "Collecting cloudpickle<3\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Using cached prometheus_flask_exporter-0.23.1-py3-none-any.whl (18 kB)\n",
      "Collecting filelock<4,>=3.12.2\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting platformdirs<5,>=3.9.1\n",
      "  Using cached platformdirs-4.0.0-py3-none-any.whl (17 kB)\n",
      "Collecting distlib<1,>=0.3.7\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /home/doha/.pyenv/versions/3.7.3/envs/event_flow/lib/python3.7/site-packages (from gunicorn<21; platform_system != \"Windows\"->mlflow->-r requirements.txt (line 12)) (68.0.0)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))))\n",
      "  Using cached greenlet-3.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (602 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: PyYAML, progress\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44638 sha256=37f924b15cc11e92e9643a469931533be9364d3af2e163a36e88585406266daa\n",
      "  Stored in directory: /home/doha/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for progress (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9612 sha256=d698048772ec12c05806af32da126941d0af8269ee1604112b32b20a075b568b\n",
      "  Stored in directory: /home/doha/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
      "Successfully built PyYAML progress\n",
      "\u001b[31mERROR: mlflow 1.30.1 has requirement importlib-metadata!=4.7.0,<6,>=3.7.0, but you'll have importlib-metadata 6.7.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 1.30.1 has requirement pytz<2023, but you'll have pytz 2024.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions, future, numpy, dataclasses, torch, pillow, torchvision, PyYAML, pytz, pandas, h5py, hdf5plugin, opencv-python, cfgv, filelock, zipp, importlib-metadata, platformdirs, distlib, virtualenv, identify, nodeenv, pre-commit, certifi, cycler, kiwisolver, pyparsing, matplotlib, progress, sqlparse, gunicorn, urllib3, idna, charset-normalizer, requests, websocket-client, packaging, docker, MarkupSafe, Werkzeug, Jinja2, itsdangerous, click, Flask, scipy, querystring-parser, greenlet, sqlalchemy, smmap, gitdb, gitpython, oauthlib, pyjwt, tabulate, databricks-cli, Mako, importlib-resources, alembic, protobuf, cloudpickle, prometheus-client, prometheus-flask-exporter, mlflow\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed Flask-2.2.5 Jinja2-3.1.4 Mako-1.2.4 MarkupSafe-2.1.5 PyYAML-5.3.1 Werkzeug-2.2.3 alembic-1.12.1 certifi-2024.8.30 cfgv-3.3.1 charset-normalizer-3.4.0 click-8.1.7 cloudpickle-2.2.1 cycler-0.11.0 databricks-cli-0.18.0 dataclasses-0.6 distlib-0.3.9 docker-6.1.3 filelock-3.12.2 future-1.0.0 gitdb-4.0.11 gitpython-3.1.43 greenlet-3.1.1 gunicorn-20.1.0 h5py-2.10.0 hdf5plugin-3.3.1 identify-2.5.24 idna-3.10 importlib-metadata-6.7.0 importlib-resources-5.12.0 itsdangerous-2.1.2 kiwisolver-1.4.5 matplotlib-3.3.2 mlflow-1.30.1 nodeenv-1.9.1 numpy-1.19.2 oauthlib-3.2.2 opencv-python-4.4.0.44 packaging-21.3 pandas-1.3.5 pillow-9.5.0 platformdirs-4.0.0 pre-commit-2.21.0 progress-1.6 prometheus-client-0.17.1 prometheus-flask-exporter-0.23.1 protobuf-4.24.4 pyjwt-2.8.0 pyparsing-3.1.4 pytz-2024.2 querystring-parser-1.2.4 requests-2.31.0 scipy-1.7.3 smmap-5.0.1 sqlalchemy-1.4.54 sqlparse-0.4.4 tabulate-0.9.0 torch-1.7.0 torchvision-0.8.0 typing-extensions-4.7.1 urllib3-2.0.7 virtualenv-20.26.6 websocket-client-1.6.1 zipp-3.15.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/doha/.pyenv/versions/3.7.3/envs/event_flow/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../../../../media/doha/SSK Drive/dataset/UZH-FPV Dataset/Indoor_forward_facing/By_DAVIS/indoor_forward_3_davis_with_gt/events.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(path, sep=\" \", header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.540820e+09</td>\n",
       "      <td>243</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.540820e+09</td>\n",
       "      <td>196</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.540820e+09</td>\n",
       "      <td>289</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.540820e+09</td>\n",
       "      <td>336</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.540820e+09</td>\n",
       "      <td>171</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1    2  3\n",
       "0  1.540820e+09  243  127  0\n",
       "1  1.540820e+09  196   25  1\n",
       "2  1.540820e+09  289   94  1\n",
       "3  1.540820e+09  336  178  1\n",
       "4  1.540820e+09  171   66  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 =\"../../../../../../media/doha/SSK Drive/dataset/MVSEC/outdoor_night3_data.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(path2, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdavis= f['davis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fright=fdavis['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['events', 'image_raw', 'image_raw_event_inds', 'image_raw_ts', 'imu', 'imu_ts']>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fright.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fevents =fright['events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset details:\n",
      "Attributes: []\n",
      "Keys: ['events', 'image_raw', 'image_raw_event_inds', 'image_raw_ts', 'imu', 'imu_ts']\n",
      "Dataset type: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset details:\")\n",
    "print(\"Attributes:\", list(fright.attrs.keys()))\n",
    "print(\"Keys:\", list(fright.keys()))\n",
    "print(\"Dataset type:\", fright[\"events\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File opened successfully.\n",
      "Navigating to 'davis/right/events' dataset...\n",
      "Dataset details:\n",
      "Shape: (102576921, 4)\n",
      "Data type (dtype): float64\n",
      "Not a compound dataset. Data type is: float64\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "path2 = \"../../../../../../media/doha/SSK Drive/dataset/MVSEC/outdoor_night3_data.hdf5\"\n",
    "\n",
    "if not os.path.exists(path2):\n",
    "    print(f\"File not found: {path2}\")\n",
    "else:\n",
    "    try:\n",
    "        with h5py.File(path2, 'r') as file:\n",
    "            print(\"File opened successfully.\")\n",
    "            \n",
    "            print(\"Navigating to 'davis/right/events' dataset...\")\n",
    "            events_dataset = file['davis']['right']['events']\n",
    "            \n",
    "            # Inspect the dataset\n",
    "            print(\"Dataset details:\")\n",
    "            print(\"Shape:\", events_dataset.shape)\n",
    "            print(\"Data type (dtype):\", events_dataset.dtype)\n",
    "            \n",
    "            # If it's a compound type, print the fields\n",
    "            if events_dataset.dtype.names:\n",
    "                print(\"Compound dataset fields:\", events_dataset.dtype.names)\n",
    "            else:\n",
    "                print(\"Not a compound dataset. Data type is:\", events_dataset.dtype)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"IOError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(events_dataset.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File opened successfully.\n",
      "Navigating to 'davis/right/events' dataset...\n",
      "Extracted event data:\n",
      "xs: [205.  51. 209. 205. 181. 115. 209. 199.  90.  39.]\n",
      "ys: [  7. 218.   7.   2.  92. 111.   3.   7.  36.  28.]\n",
      "ts: [1.50491663e+09 1.50491663e+09 1.50491663e+09 1.50491663e+09\n",
      " 1.50491663e+09 1.50491663e+09 1.50491663e+09 1.50491663e+09\n",
      " 1.50491663e+09 1.50491663e+09]\n",
      "ps: [-1.  1. -1. -1. -1.  1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "path2 = \"../../../../../../media/doha/SSK Drive/dataset/MVSEC/outdoor_night3_data.hdf5\"\n",
    "\n",
    "if not os.path.exists(path2):\n",
    "    print(f\"File not found: {path2}\")\n",
    "else:\n",
    "    try:\n",
    "        with h5py.File(path2, 'r') as file:\n",
    "            print(\"File opened successfully.\")\n",
    "            \n",
    "            # Navigate to the 'events' dataset\n",
    "            print(\"Navigating to 'davis/right/events' dataset...\")\n",
    "            events_dataset = file['davis']['right']['events']\n",
    "            \n",
    "            # Extract a slice of the dataset (first 100 events)\n",
    "            idx0, idx1 = 0, 100  # Define the range of indices to extract\n",
    "            events_slice = events_dataset[idx0:idx1]\n",
    "            \n",
    "            # Split the slice into individual components\n",
    "            xs = events_slice[:, 0]  # First column: x-coordinates\n",
    "            ys = events_slice[:, 1]  # Second column: y-coordinates\n",
    "            ts = events_slice[:, 2]  # Third column: timestamps\n",
    "            ps = events_slice[:, 3]  # Fourth column: polarities\n",
    "\n",
    "            # Print the extracted data for verification\n",
    "            print(\"Extracted event data:\")\n",
    "            print(\"xs:\", xs[:10])  # Print first 10 x-coordinates\n",
    "            print(\"ys:\", ys[:10])  # Print first 10 y-coordinates\n",
    "            print(\"ts:\", ts[:10])  # Print first 10 timestamps\n",
    "            print(\"ps:\", ps[:10])  # Print first 10 polarities\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"IOError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09,\n",
       "       1.50491663e+09, 1.50491663e+09, 1.50491663e+09, 1.50491663e+09])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class H5Loader:\n",
    "    def __init__(self, config, num_bins, round_encoding=False):\n",
    "        self.config = config\n",
    "        self.num_bins = num_bins\n",
    "        self.round_encoding = round_encoding\n",
    "        self.last_proc_timestamp = 0\n",
    "\n",
    "        # \"memory\" that goes from forward pass to the next\n",
    "        self.batch_idx = [i for i in range(self.config[\"loader\"][\"batch_size\"])]  # event sequence\n",
    "        self.batch_row = [0 for i in range(self.config[\"loader\"][\"batch_size\"])]  # event_idx / time_idx / frame_idx / gt_idx\n",
    "\n",
    "        # input event sequences\n",
    "        self.files = []\n",
    "        for root, dirs, files in os.walk(config[\"data\"][\"path\"]):\n",
    "            for file in files:\n",
    "                if file.endswith(\".h5\"):\n",
    "                    self.files.append(os.path.join(root, file))\n",
    "\n",
    "        if not self.files:\n",
    "            raise ValueError(\"No files found in the specified path.\")\n",
    "\n",
    "        # open first files\n",
    "        self.open_files = []\n",
    "        self.batch_last_ts = []\n",
    "        for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "            self.open_files.append(h5py.File(self.files[batch], \"r\"))\n",
    "            self.batch_last_ts.append(self.open_files[-1][\"events/ts\"][-1] - self.open_files[-1].attrs[\"t0\"])\n",
    "\n",
    "        # load frames from open files\n",
    "        self.open_files_frames = []\n",
    "        if self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                frames = Frames()\n",
    "                self.open_files[batch][\"images\"].visititems(frames)\n",
    "                self.open_files_frames.append(frames)\n",
    "\n",
    "        # load GT optical flow maps from open files\n",
    "        self.open_files_flowmaps = []\n",
    "        if config[\"data\"][\"mode\"] == \"gtflow_dt1\" or config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                flowmaps = FlowMaps()\n",
    "                if config[\"data\"][\"mode\"] == \"gtflow_dt1\":\n",
    "                    self.open_files[batch][\"flow_dt1\"].visititems(flowmaps)\n",
    "                elif config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "                    self.open_files[batch][\"flow_dt4\"].visititems(flowmaps)\n",
    "                self.open_files_flowmaps.append(flowmaps)\n",
    "\n",
    "        # progress bars\n",
    "        if self.config[\"vis\"][\"bars\"]:\n",
    "            self.open_files_bar = []\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                max_iters = self.get_iters(batch)\n",
    "                self.open_files_bar.append(ProgressBar(self.files[batch].split(\"/\")[-1], max=max_iters))\n",
    "\n",
    "    def get_iters(self, batch):\n",
    "        \"\"\"\n",
    "        Compute the number of forward passes given a sequence and an input mode and window.\n",
    "        \"\"\"\n",
    "        if self.config[\"data\"][\"mode\"] == \"events\":\n",
    "            max_iters = len(self.open_files[batch][\"events/xs\"])\n",
    "        elif self.config[\"data\"][\"mode\"] == \"time\":\n",
    "            max_iters = self.open_files[batch].attrs[\"duration\"]\n",
    "        elif self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            max_iters = len(self.open_files_frames[batch].ts) - 1\n",
    "        elif self.config[\"data\"][\"mode\"] == \"gtflow_dt1\" or self.config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            max_iters = len(self.open_files_flowmaps[batch].ts) - 1\n",
    "        else:\n",
    "            print(\"DataLoader error: Unknown mode.\")\n",
    "            raise AttributeError\n",
    "\n",
    "        return max_iters // self.config[\"data\"][\"window\"]\n",
    "\n",
    "    def get_events(self, file, idx0, idx1):\n",
    "        \"\"\"\n",
    "        Get all the events in between two indices.\n",
    "        :param file: file to read from\n",
    "        :param idx0: start index\n",
    "        :param idx1: end index\n",
    "        :return xs: [N] numpy array with event x location\n",
    "        :return ys: [N] numpy array with event y location\n",
    "        :return ts: [N] numpy array with event timestamp\n",
    "        :return ps: [N] numpy array with event polarity ([-1, 1])\n",
    "        \"\"\"\n",
    "        xs = file[\"events/xs\"][idx0:idx1]\n",
    "        ys = file[\"events/ys\"][idx0:idx1]\n",
    "        ts = file[\"events/ts\"][idx0:idx1]\n",
    "        ps = file[\"events/ps\"][idx0:idx1]\n",
    "        ts -= file.attrs[\"t0\"]  # sequence starting at t0 = 0\n",
    "        if ts.shape[0] > 0:\n",
    "            self.last_proc_timestamp = ts[-1]\n",
    "        return xs, ys, ts, ps\n",
    "\n",
    "    def get_event_index(self, batch, window=0):\n",
    "        \"\"\"\n",
    "        Get all the event indices to be used for reading.\n",
    "        :param batch: batch index\n",
    "        :param window: input window\n",
    "        :return event_idx: event index\n",
    "        \"\"\"\n",
    "        event_idx0 = None\n",
    "        event_idx1 = None\n",
    "        if self.config[\"data\"][\"mode\"] == \"events\":\n",
    "            event_idx0 = self.batch_row[batch]\n",
    "            event_idx1 = self.batch_row[batch] + window\n",
    "        elif self.config[\"data\"][\"mode\"] == \"time\":\n",
    "            event_idx0 = self.find_ts_index(\n",
    "                self.open_files[batch], self.batch_row[batch] + self.open_files[batch].attrs[\"t0\"]\n",
    "            )\n",
    "            event_idx1 = self.find_ts_index(\n",
    "                self.open_files[batch], self.batch_row[batch] + self.open_files[batch].attrs[\"t0\"] + window\n",
    "            )\n",
    "        elif self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            idx0 = int(np.floor(self.batch_row[batch]))\n",
    "            idx1 = int(np.ceil(self.batch_row[batch] + window))\n",
    "            if window < 1.0 and idx1 - idx0 > 1:\n",
    "                idx0 += idx1 - idx0 - 1\n",
    "            event_idx0 = self.find_ts_index(self.open_files[batch], self.open_files_frames[batch].ts[idx0])\n",
    "            event_idx1 = self.find_ts_index(self.open_files[batch], self.open_files_frames[batch].ts[idx1])\n",
    "        elif self.config[\"data\"][\"mode\"] == \"gtflow_dt1\" or self.config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            idx0 = int(np.floor(self.batch_row[batch]))\n",
    "            idx1 = int(np.ceil(self.batch_row[batch] + window))\n",
    "            if window < 1.0 and idx1 - idx0 > 1:\n",
    "                idx0 += idx1 - idx0 - 1\n",
    "            event_idx0 = self.find_ts_index(self.open_files[batch], self.open_files_flowmaps[batch].ts[idx0])\n",
    "            event_idx1 = self.find_ts_index(self.open_files[batch], self.open_files_flowmaps[batch].ts[idx1])\n",
    "        else:\n",
    "            print(\"DataLoader error: Unknown mode.\")\n",
    "            raise AttributeError\n",
    "        return event_idx0, event_idx1\n",
    "\n",
    "    def find_ts_index(self, file, timestamp):\n",
    "        \"\"\"\n",
    "        Find closest event index for a given timestamp through binary search.\n",
    "        \"\"\"\n",
    "        return binary_search_array(file[\"events/ts\"], timestamp)\n",
    "    def __init__(self, config, num_bins, round_encoding=False):\n",
    "        self.config = config\n",
    "        self.num_bins = num_bins\n",
    "        self.round_encoding = round_encoding\n",
    "        self.last_proc_timestamp = 0\n",
    "\n",
    "        # \"memory\" that goes from forward pass to the next\n",
    "        self.batch_idx = [i for i in range(self.config[\"loader\"][\"batch_size\"])]  # event sequence\n",
    "        self.batch_row = [0 for i in range(self.config[\"loader\"][\"batch_size\"])]  # event_idx / time_idx / frame_idx / gt_idx\n",
    "\n",
    "        # input event sequences\n",
    "        self.files = []\n",
    "        for root, dirs, files in os.walk(config[\"data\"][\"path\"]):\n",
    "            for file in files:\n",
    "                if file.endswith(\".h5\"):\n",
    "                    self.files.append(os.path.join(root, file))\n",
    "\n",
    "        if not self.files:\n",
    "            raise ValueError(\"No files found in the specified path.\")\n",
    "\n",
    "        # open first files\n",
    "        self.open_files = []\n",
    "        self.batch_last_ts = []\n",
    "        for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "            self.open_files.append(h5py.File(self.files[batch], \"r\"))\n",
    "            self.batch_last_ts.append(self.open_files[-1][\"events/ts\"][-1] - self.open_files[-1].attrs[\"t0\"])\n",
    "\n",
    "        # load frames from open files\n",
    "        self.open_files_frames = []\n",
    "        if self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                frames = Frames()\n",
    "                self.open_files[batch][\"images\"].visititems(frames)\n",
    "                self.open_files_frames.append(frames)\n",
    "\n",
    "        # load GT optical flow maps from open files\n",
    "        self.open_files_flowmaps = []\n",
    "        if config[\"data\"][\"mode\"] == \"gtflow_dt1\" or config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                flowmaps = FlowMaps()\n",
    "                if config[\"data\"][\"mode\"] == \"gtflow_dt1\":\n",
    "                    self.open_files[batch][\"flow_dt1\"].visititems(flowmaps)\n",
    "                elif config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "                    self.open_files[batch][\"flow_dt4\"].visititems(flowmaps)\n",
    "                self.open_files_flowmaps.append(flowmaps)\n",
    "\n",
    "        # progress bars\n",
    "        if self.config[\"vis\"][\"bars\"]:\n",
    "            self.open_files_bar = []\n",
    "            for batch in range(self.config[\"loader\"][\"batch_size\"]):\n",
    "                max_iters = self.get_iters(batch)\n",
    "                self.open_files_bar.append(ProgressBar(self.files[batch].split(\"/\")[-1], max=max_iters))\n",
    "\n",
    "    def get_iters(self, batch):\n",
    "        \"\"\"\n",
    "        Compute the number of forward passes given a sequence and an input mode and window.\n",
    "        \"\"\"\n",
    "        if self.config[\"data\"][\"mode\"] == \"events\":\n",
    "            max_iters = len(self.open_files[batch][\"events/xs\"])\n",
    "        elif self.config[\"data\"][\"mode\"] == \"time\":\n",
    "            max_iters = self.open_files[batch].attrs[\"duration\"]\n",
    "        elif self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            max_iters = len(self.open_files_frames[batch].ts) - 1\n",
    "        elif self.config[\"data\"][\"mode\"] == \"gtflow_dt1\" or self.config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            max_iters = len(self.open_files_flowmaps[batch].ts) - 1\n",
    "        else:\n",
    "            print(\"DataLoader error: Unknown mode.\")\n",
    "            raise AttributeError\n",
    "\n",
    "        return max_iters // self.config[\"data\"][\"window\"]\n",
    "\n",
    "    def get_events(self, file, idx0, idx1):\n",
    "        \"\"\"\n",
    "        Get all the events in between two indices.\n",
    "        :param file: file to read from\n",
    "        :param idx0: start index\n",
    "        :param idx1: end index\n",
    "        :return xs: [N] numpy array with event x location\n",
    "        :return ys: [N] numpy array with event y location\n",
    "        :return ts: [N] numpy array with event timestamp\n",
    "        :return ps: [N] numpy array with event polarity ([-1, 1])\n",
    "        \"\"\"\n",
    "        xs = file[\"events/xs\"][idx0:idx1]\n",
    "        ys = file[\"events/ys\"][idx0:idx1]\n",
    "        ts = file[\"events/ts\"][idx0:idx1]\n",
    "        ps = file[\"events/ps\"][idx0:idx1]\n",
    "        ts -= file.attrs[\"t0\"]  # sequence starting at t0 = 0\n",
    "        if ts.shape[0] > 0:\n",
    "            self.last_proc_timestamp = ts[-1]\n",
    "        return xs, ys, ts, ps\n",
    "\n",
    "    def get_event_index(self, batch, window=0):\n",
    "        \"\"\"\n",
    "        Get all the event indices to be used for reading.\n",
    "        :param batch: batch index\n",
    "        :param window: input window\n",
    "        :return event_idx: event index\n",
    "        \"\"\"\n",
    "        event_idx0 = None\n",
    "        event_idx1 = None\n",
    "        if self.config[\"data\"][\"mode\"] == \"events\":\n",
    "            event_idx0 = self.batch_row[batch]\n",
    "            event_idx1 = self.batch_row[batch] + window\n",
    "        elif self.config[\"data\"][\"mode\"] == \"time\":\n",
    "            event_idx0 = self.find_ts_index(\n",
    "                self.open_files[batch], self.batch_row[batch] + self.open_files[batch].attrs[\"t0\"]\n",
    "            )\n",
    "            event_idx1 = self.find_ts_index(\n",
    "                self.open_files[batch], self.batch_row[batch] + self.open_files[batch].attrs[\"t0\"] + window\n",
    "            )\n",
    "        elif self.config[\"data\"][\"mode\"] == \"frames\":\n",
    "            idx0 = int(np.floor(self.batch_row[batch]))\n",
    "            idx1 = int(np.ceil(self.batch_row[batch] + window))\n",
    "            if window < 1.0 and idx1 - idx0 > 1:\n",
    "                idx0 += idx1 - idx0 - 1\n",
    "            event_idx0 = self.find_ts_index(self.open_files[batch], self.open_files_frames[batch].ts[idx0])\n",
    "            event_idx1 = self.find_ts_index(self.open_files[batch], self.open_files_frames[batch].ts[idx1])\n",
    "        elif self.config[\"data\"][\"mode\"] == \"gtflow_dt1\" or self.config[\"data\"][\"mode\"] == \"gtflow_dt4\":\n",
    "            idx0 = int(np.floor(self.batch_row[batch]))\n",
    "            idx1 = int(np.ceil(self.batch_row[batch] + window))\n",
    "            if window < 1.0 and idx1 - idx0 > 1:\n",
    "                idx0 += idx1 - idx0 - 1\n",
    "            event_idx0 = self.find_ts_index(self.open_files[batch], self.open_files_flowmaps[batch].ts[idx0])\n",
    "            event_idx1 = self.find_ts_index(self.open_files[batch], self.open_files_flowmaps[batch].ts[idx1])\n",
    "        else:\n",
    "            print(\"DataLoader error: Unknown mode.\")\n",
    "            raise AttributeError\n",
    "        return event_idx0, event_idx1\n",
    "\n",
    "    def find_ts_index(self, file, timestamp):\n",
    "        \"\"\"\n",
    "        Find closest event index for a given timestamp through binary search.\n",
    "        \"\"\"\n",
    "        return binary_search_array(file[\"events/ts\"], timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the H5Loader class\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"../../../../../../media/doha/SSK Drive/dataset/MVSEC/outdoor_night2_gt.hdf5\",\n",
    "        \"mode\": \"gtflow_dt1\",\n",
    "        \"window\": 1,\n",
    "        \"window_eval\": 15000,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"mask_output\": True,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"name\": [\"AEE\"],\n",
    "        \"flow_scaling\": 128,\n",
    "    },\n",
    "    \"loader\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"resolution\": [256, 256],\n",
    "        \"augment\": [],\n",
    "        \"gpu\": 0,\n",
    "    },\n",
    "    \"vis\": {\n",
    "        \"enabled\": True,\n",
    "        \"px\": 400,\n",
    "        \"bars\": True,\n",
    "        \"activity\": False,\n",
    "        \"store\": False,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No files found in the specified path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3682274/3621684371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create an instance of H5Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH5Loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Test the loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3682274/60458508.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, num_bins, round_encoding)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No files found in the specified path.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# open first files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No files found in the specified path."
     ]
    }
   ],
   "source": [
    "num_bins = 5  # Example value for num_bins\n",
    "\n",
    "# Create an instance of H5Loader\n",
    "loader = H5Loader(config, num_bins)\n",
    "\n",
    "# Test the loader\n",
    "print(\"Files loaded:\", loader.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def get_events(file, idx0, idx1):\n",
    "        \"\"\"\n",
    "        Get all the events in between two indices.\n",
    "        :param file: file to read from\n",
    "        :param idx0: start index\n",
    "        :param idx1: end index\n",
    "        :return xs: [N] numpy array with event x location\n",
    "        :return ys: [N] numpy array with event y location\n",
    "        :return ts: [N] numpy array with event timestamp\n",
    "        :return ps: [N] numpy array with event polarity ([-1, 1])\n",
    "        \"\"\"\n",
    "        xs = file[\"events/xs\"][idx0:idx1]\n",
    "        ys = file[\"events/ys\"][idx0:idx1]\n",
    "        ts = file[\"events/ts\"][idx0:idx1]\n",
    "        ps = file[\"events/ps\"][idx0:idx1]\n",
    "        ts -= file.attrs[\"t0\"]  # sequence starting at t0 = 0\n",
    "        if ts.shape[0] > 0:\n",
    "            self.last_proc_timestamp = ts[-1]\n",
    "        return xs, ys, ts, ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File opened successfully.\n",
      "Navigating to 'davis' group...\n",
      "Successfully navigated to 'davis' group.\n",
      "Navigating to 'right' group...\n",
      "Successfully navigated to 'right' group.\n",
      "Navigating to 'events' group...\n",
      "Successfully navigated to 'events' group.\n",
      "Fetching event data...\n",
      "An error occurred: Field names only allowed for compound types\n",
      "An error occurred while opening the file: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def get_events(file, idx0, idx1):\n",
    "    \"\"\"\n",
    "    Get all the events in between two indices.\n",
    "    :param file: file to read from\n",
    "    :param idx0: start index\n",
    "    :param idx1: end index\n",
    "    :return xs: [N] numpy array with event x location\n",
    "    :return ys: [N] numpy array with event y location\n",
    "    :return ts: [N] numpy array with event timestamp\n",
    "    :return ps: [N] numpy array with event polarity ([-1, 1])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Navigating to 'davis' group...\")\n",
    "        events_group = file['davis']\n",
    "        print(\"Successfully navigated to 'davis' group.\")\n",
    "\n",
    "        print(\"Navigating to 'right' group...\")\n",
    "        events_group = events_group['right']\n",
    "        print(\"Successfully navigated to 'right' group.\")\n",
    "        \n",
    "        print(\"Navigating to 'events' group...\")\n",
    "        events_group = events_group['events']\n",
    "        print(\"Successfully navigated to 'events' group.\")\n",
    "\n",
    "        print(\"Fetching event data...\")\n",
    "        xs = events_group[\"xs\"][idx0:idx1]\n",
    "        ys = events_group[\"ys\"][idx0:idx1]\n",
    "        ts = events_group[\"ts\"][idx0:idx1]\n",
    "        ps = events_group[\"ps\"][idx0:idx1]\n",
    "        ts -= events_group.attrs[\"t0\"]  # sequence starting at t0 = 0\n",
    "        print(\"Successfully fetched event data.\")\n",
    "        \n",
    "        return xs, ys, ts, ps\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        print(\"Available keys in the file:\", list(events_group.keys()))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage example\n",
    "path2 = \"../../../../../../media/doha/SSK Drive/dataset/MVSEC/outdoor_night3_data.hdf5\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(path2):\n",
    "    print(f\"File not found: {path2}\")\n",
    "else:\n",
    "    try:\n",
    "        with h5py.File(path2, 'r') as f:\n",
    "            print(\"File opened successfully.\")\n",
    "            xs, ys, ts, ps = get_events(f, 0, 100)\n",
    "            print(\"Event data:\", xs, ys, ts, ps)\n",
    "    except IOError as e:\n",
    "        print(f\"IOError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while opening the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
